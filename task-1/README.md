# Task 1: CUDA Programming

High performance gemm implementation on Nvidia A100 ([internal feishu doc](https://aicarrier.feishu.cn/wiki/EvivwNtVRij2XVk0i36cBN8Bn1f)).

## 1. ğŸ¯Target

Implement a high performance gemm (General Matrix Multiply) function with CUDA on Nvidia A100 for float32 and float16 data types.

The implementation should be able to achieve at least 90% of the performance of cuBLAS, with the given benchmarking structure.

## 2. Benchmark cBlas and cuBlas

Install OpenBLAS before running:

```bash
apt update && apt install -y libopenblas-dev
```

Build test executables:

```bash
# Build gemm implemented with cblas with float32 as dtype:
bash scripts/build-task1.sh -v0 -f32
# Build gemm implemented with cblas with float16 as dtype:
bash scripts/build-task1.sh -v0 -f16
# Build gemm implemented with cublas with float32 as dtype:
bash scripts/build-task1.sh -v1 -f32
# Build gemm implemented with cublas with float16 as dtype:
bash scripts/build-task1.sh -v1 -f16
```

> ğŸ’¡**Note**:  
> It is suggested to restart clangd server after building (to avoid some code analysis errors).  
> To restart clangd server, press `Ctrl+Shift+P` in VSCode, and select `clangd: Restart language server`.  
> ![restart-clangd](../docs/imgs/restart-clangd.png)

Run the executables in "[./build/src](../build/src)" directory to get the benchmark results.

## 3. Add Your Own Implementation

Create a `.cu` file in "[./src](./src)" directory with any name you like, and implement the function `matmul` with a proper playground matmul signature.

For example, add following lines in "./src/pjlab/bigchip/f16-v2.cu" to provide the definition for function `matmul<float16_t, 2>`:

```cpp
// @file: ./task-1/src/pjlab/bigchip/f16-v2.cu

#include "playground/matmul.hpp"

namespace playground {
// Implement the matmul function with DType=float16_t and Version=2
PLAYGROUND_MATMUL_SIG(float16_t, 2, A, B, C, M, N, K)
{
    // ......
}
}
```

> ğŸ’¡**Note**:  
> - Do not use version 0 and 1 because they are for cblas and cublas respectively. The version must be a `uint8_t`.  

Now you can build an new executable to test your implementation with the following command:

```bash
# Build the test binary with DType=float16 and Version=2:
bash scripts/build.sh -v2 -f16
```

## 4.Example final results

### CUDA Core(FP32)
| Version | v0 | v1 | v2 | v3 | v4 | cuBLAS | Theory Peak |
| --- | --- | --- | --- | --- | --- | --- | --- | 
| Average error | 0.0115 | 0.0115 | 0.0115 | 0.0116 | 0.0116 | / | / |
| TFLOPS | 2.41 | 3.85 | 9.24 | 15.15 | 17.16 | 18.38 | 19.5 |

### Tensor Core(FP16)

| Version | v0 | v1 | v2 |  v3 |v4 | cuBLAS | Theory Peak |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Average error | 0.0117 | 0.0117 | 0.0117 | 0.0117 | 0.0019 |0.0153 | / |
| TFLOPS | 18.09 | 53.05 |103.05 |159.35 | 213.12 |222.11 | 312 |

> ğŸ’¡**Note**:  
> Some card can reach above 250 TFLOPS using cuBLAS fp16. The target is the 90% of cuBLAS on the same card

## 5. References
See also: [feishu doc: cudaå­¦ä¹ èµ„æ–™](https://aicarrier.feishu.cn/wiki/SFdnw61vHi1AfRkeJVecgMjBnrc)

### CUDA Core

- "Programming Massively Parallel Processors  A Hands-on Approach (Fourth Edition)" Chapter 2-3

- "Programming Massively Parallel Processors  A Hands-on Approach (Fourth Edition)" Chapter 4-5
- [CUDAç¼–ç¨‹å…¥é—¨åŠä¼˜åŒ–](https://zhuanlan.zhihu.com/p/441146275) 1.2 Thread Block Tile: åˆ©ç”¨ Shared Memory å‡å°‘é‡å¤è®¿å­˜

- "Programming Massively Parallel Processors  A Hands-on Approach (Fourth Edition)" Chapter 6-6.3 Thread coarsening
- [how-to-optimize-gemm](https://zhuanlan.zhihu.com/p/478846788) MMult_cuda_4 & MMult_cuda_5
- [CUDA çŸ©é˜µä¹˜æ³•ç»ˆæä¼˜åŒ–æŒ‡å—](https://zhuanlan.zhihu.com/p/410278370) Naive å®ç°çš„åˆ†æï¼šåˆ°åº•å·®åœ¨å“ªé‡Œï¼Ÿ

- "Programming Massively Parallel Processors  A Hands-on Approach (Fourth Edition)" Chapter 6-6.1 Memory coalescing, 6.2 Hiding memory latency
- [how-to-optimize-gemm](https://zhuanlan.zhihu.com/p/478846788) MMult_cuda_9
- [cuda/MMult_cuda_9.cu](https://github.com/tpoisonooo/how-to-optimize-gemm/blob/master/cuda/MMult_cuda_9.cu)
- [CUDA çŸ©é˜µä¹˜æ³•ç»ˆæä¼˜åŒ–æŒ‡å—](https://zhuanlan.zhihu.com/p/410278370) æè‡´çš„è®¿å­˜ä¼˜åŒ–
- [CUDAç¼–ç¨‹å…¥é—¨åŠä¼˜åŒ–](https://zhuanlan.zhihu.com/p/441146275) 1.3 Warp Tile ä¸ Thread Tile: åˆ©ç”¨å¯„å­˜å™¨æ¶ˆé™¤ Shared Memory ç“¶é¢ˆ


- [how-to-optimize-gemm](https://zhuanlan.zhihu.com/p/478846788) MMult_cuda_12
- [cuda/MMult_cuda_12.cu](https://github.com/tpoisonooo/how-to-optimize-gemm/blob/master/cuda/MMult_cuda_12.cu)
- [CUDAç¼–ç¨‹å…¥é—¨åŠä¼˜åŒ–](https://zhuanlan.zhihu.com/p/441146275) 1.4 Double Buffer: è®© GEMM æµæ°´å¹¶è¡Œèµ·æ¥

### Tensor Core

- [cudaå­¦ä¹ ï¼šå­¦ä¹ nvcuda::wmmaå®ç°é«˜æ•ˆgemm](https://zhuanlan.zhihu.com/p/353208013) simple version

- [cudaå­¦ä¹ ï¼šå­¦ä¹ nvcuda::wmmaå®ç°é«˜æ•ˆgemm](https://zhuanlan.zhihu.com/p/353208013) sample version with detailed annotations
- [Official sample provided by NVIDIA](https://github.com/NVIDIA/cuda-samples/blob/master/Samples/3_CUDA_Features/cudaTensorCoreGemm/cudaTensorCoreGemm.cu)

- [Nvidia Tensor Core-CUDA HGEMMä¼˜åŒ–è¿›é˜¶](https://zhuanlan.zhihu.com/p/639297098/) 4.5 æé«˜L2 Cacheå‘½ä¸­ç‡
- [ä¸€æ­¥æ­¥ä¼˜åŒ– GEMM by Tensorcore](https://zhuanlan.zhihu.com/p/638522893) è°ƒæ•´çº¿ç¨‹å—åˆ†é…åˆ°çš„è®¡ç®—ä½ç½®(swizzle)


#### Source code:
- [src/wmma/wmma_async_stage3.cu](https://github.com/Bruce-Lee-LY/cuda_hgemm/blob/master/src/wmma/wmma_async_stage3.cu) 3 stages pipeline with WMMA API

Asynchronous data copy:
- [ Data Movement and Conversion Instructions: cp.async](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async) To know the usage of cp.async instructions
- [Performance Guidance for memcpy_async](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async) To know the usage of asynchronous data copy

#### Multi-buffer with prefetching:
- [Nvidia Tensor Core-CUDA HGEMMä¼˜åŒ–è¿›é˜¶](https://zhuanlan.zhihu.com/p/639297098) 5 Pipelineä¼˜åŒ–-5.2 Stage
- [ä¸€æ­¥æ­¥ä¼˜åŒ– GEMM by Tensorcore](https://zhuanlan.zhihu.com/p/638522893) ä½¿ç”¨æ•°æ®é¢„å–(prefetch)

#### Permute to use memory coalescing and avoid bank conflicts:
- [cudaï¼ˆcutlassï¼‰ç¼–ç¨‹ä¹‹swizzle](https://www.bilibili.com/video/BV1Jb421e7UN/?spm_id_from=333.999.0.0&vd_source=2fe7991a33356057a2e41a2d37f9b7e0) A more detailed video explanation of swizzle based on CUTLASS

### For Further Study

- [åŸºäº CUTE çš„ GEMM ä¼˜åŒ–ã€1ã€‘â€”â€” Baseline å®ç°](https://zhuanlan.zhihu.com/p/695063154)
- [åŸºäº CUTE çš„ GEMM ä¼˜åŒ–ã€2ã€‘â€”â€” é«˜æ•ˆ GEMM å®ç°ï¼Œè¶…è¶Š Cublas 20%](https://zhuanlan.zhihu.com/p/696028389)
- [cuteç³»åˆ—è®²è§£](https://www.zhihu.com/people/reed-84-49/posts)

